#pragma once

#include "carry_chain.cuh"
#include "ff_config.cuh"

#define HOST_INLINE __host__ __forceinline__
#define DEVICE_INLINE __device__ __forceinline__
#define HOST_DEVICE_INLINE __host__ __device__ __forceinline__

template <class FF_CONFIG, const uint32_t &INV> struct ff_dispatch_st {
  // allows consumers to access the underlying config (e.g., "fd_q::CONFIG") if needed
  using CONFIG = FF_CONFIG;

  static constexpr int LPT = CONFIG::limbs_count;
  static constexpr int TPF = 1;
  static constexpr unsigned TLC = CONFIG::limbs_count;

  typedef ff_storage<TLC> storage;
  typedef ff_storage_wide<TLC> storage_wide;

  // return number of bits in modulus
  static constexpr unsigned MBC = CONFIG::modulus_bits_count;

  // return modulus
  template <unsigned MULTIPLIER = 1> static constexpr HOST_DEVICE_INLINE storage get_modulus() {
    switch (MULTIPLIER) {
    case 1:
      return CONFIG::modulus;
    case 2:
      return CONFIG::modulus_2;
    case 4:
      return CONFIG::modulus_4;
    default:
      return {};
    }
  }

  // return modulus^2, helpful for ab +/- cd
  template <unsigned MULTIPLIER = 1> static constexpr HOST_DEVICE_INLINE storage_wide get_modulus_squared() {
    switch (MULTIPLIER) {
    case 1:
      return CONFIG::modulus_squared;
    case 2:
      return CONFIG::modulus_squared_2;
    case 4:
      return CONFIG::modulus_squared_4;
    default:
      return {};
    }
  }

  // return r^2
  static constexpr HOST_DEVICE_INLINE storage get_r2() { return CONFIG::r2; }

  // return one in montgomery form
  static constexpr HOST_DEVICE_INLINE storage get_one() { return CONFIG::one; }

  // add or subtract limbs
#ifdef __CUDA_ARCH__
  template <bool SUBTRACT, bool CARRY_OUT> static constexpr DEVICE_INLINE uint32_t add_sub_limbs_device(const storage &xs, const storage &ys, storage &rs) {
    const uint32_t *x = xs.limbs;
    const uint32_t *y = ys.limbs;
    uint32_t *r = rs.limbs;
    carry_chain<CARRY_OUT ? TLC + 1 : TLC> chain;
#pragma unroll
    for (unsigned i = 0; i < TLC; i++)
      r[i] = SUBTRACT ? chain.sub(x[i], y[i]) : chain.add(x[i], y[i]);
    if (!CARRY_OUT)
      return 0;
    return SUBTRACT ? chain.sub(0, 0) : chain.add(0, 0);
  }

  // If we want, we could make "2*TLC" a template parameter to deduplicate with "storage" overload, but that's a minor issue.
  template <bool SUBTRACT, bool CARRY_OUT>
  static constexpr DEVICE_INLINE uint32_t add_sub_limbs_device(const storage_wide &xs, const storage_wide &ys, storage_wide &rs) {
    const uint32_t *x = xs.limbs;
    const uint32_t *y = ys.limbs;
    uint32_t *r = rs.limbs;
    carry_chain<CARRY_OUT ? 2 * TLC + 1 : 2 * TLC> chain;
#pragma unroll
    for (unsigned i = 0; i < 2 * TLC; i++) {
      r[i] = SUBTRACT ? chain.sub(x[i], y[i]) : chain.add(x[i], y[i]);
    }
    if (!CARRY_OUT)
      return 0;
    return SUBTRACT ? chain.sub(0, 0) : chain.add(0, 0);
  }
#endif

  template <bool SUBTRACT, bool CARRY_OUT> static constexpr HOST_INLINE uint32_t add_sub_limbs_host(const storage &xs, const storage &ys, storage &rs) {
    const uint32_t *x = xs.limbs;
    const uint32_t *y = ys.limbs;
    uint32_t *r = rs.limbs;
    uint32_t carry = 0;
    carry_chain<TLC, false, CARRY_OUT> chain;
    for (unsigned i = 0; i < TLC; i++)
      r[i] = SUBTRACT ? chain.sub(x[i], y[i], carry) : chain.add(x[i], y[i], carry);
    return CARRY_OUT ? carry : 0;
  }

  template <bool SUBTRACT, bool CARRY_OUT, typename T> static constexpr HOST_DEVICE_INLINE uint32_t add_sub_limbs(const T &xs, const T &ys, T &rs) {
    // No need for static_assert(std::is_same<T, storage>::value || std::is_same<T, storage_wide>::value).
    // Instantiation will fail if appropriate add_sub_limbs_device overload does not exist.
#ifdef __CUDA_ARCH__
    return add_sub_limbs_device<SUBTRACT, CARRY_OUT>(xs, ys, rs);
#else
    return add_sub_limbs_host<SUBTRACT, CARRY_OUT>(xs, ys, rs);
#endif
  }

  template <bool CARRY_OUT, typename T> static constexpr HOST_DEVICE_INLINE uint32_t add_limbs(const T &xs, const T &ys, T &rs) {
    return add_sub_limbs<false, CARRY_OUT>(xs, ys, rs);
  }

  template <bool CARRY_OUT, typename T> static constexpr HOST_DEVICE_INLINE uint32_t sub_limbs(const T &xs, const T &ys, T &rs) {
    return add_sub_limbs<true, CARRY_OUT>(xs, ys, rs);
  }

  // return xs == 0 with field operands
#ifdef __CUDA_ARCH__
  static constexpr DEVICE_INLINE bool is_zero_device(const storage &xs) {
    const uint32_t *x = xs.limbs;
    uint32_t limbs_or = x[0];
#pragma unroll
    for (unsigned i = 1; i < TLC; i++)
      limbs_or |= x[i];
    return limbs_or == 0;
  }
#endif

  static constexpr HOST_INLINE bool is_zero_host(const storage &xs) {
    for (unsigned i = 0; i < TLC; i++)
      if (xs.limbs[i])
        return false;
    return true;
  }

  static constexpr HOST_DEVICE_INLINE bool is_zero(const storage &xs) {
#ifdef __CUDA_ARCH__
    return is_zero_device(xs);
#else
    return is_zero_host(xs);
#endif
  }

  // return xs == ys with field operands
#ifdef __CUDA_ARCH__
  static constexpr DEVICE_INLINE bool eq_device(const storage &xs, const storage &ys) {
    const uint32_t *x = xs.limbs;
    const uint32_t *y = ys.limbs;
    uint32_t limbs_or = x[0] ^ y[0];
#pragma unroll
    for (unsigned i = 1; i < TLC; i++)
      limbs_or |= x[i] ^ y[i];
    return limbs_or == 0;
  }
#endif

  static constexpr HOST_INLINE bool eq_host(const storage &xs, const storage &ys) {
    for (unsigned i = 0; i < TLC; i++)
      if (xs.limbs[i] != ys.limbs[i])
        return false;
    return true;
  }

  static constexpr HOST_DEVICE_INLINE bool eq(const storage &xs, const storage &ys) {
#ifdef __CUDA_ARCH__
    return eq_device(xs, ys);
#else
    return eq_host(xs, ys);
#endif
  }

  template <unsigned REDUCTION_SIZE = 1> static constexpr HOST_DEVICE_INLINE storage reduce(const storage &xs) {
    if (REDUCTION_SIZE == 0)
      return xs;
    const storage modulus = get_modulus<REDUCTION_SIZE>();
    storage rs = {};
    return sub_limbs<true>(xs, modulus, rs) ? xs : rs;
  }

  template <unsigned REDUCTION_SIZE = 1> static constexpr HOST_DEVICE_INLINE storage_wide reduce_wide(const storage_wide &xs) {
    if (REDUCTION_SIZE == 0)
      return xs;
    const storage_wide modulus_squared = get_modulus_squared<REDUCTION_SIZE>();
    storage_wide rs = {};
    return sub_limbs<true>(xs, modulus_squared, rs) ? xs : rs;
  }

  // return xs + ys with field operands
  template <unsigned REDUCTION_SIZE = 1> static constexpr HOST_DEVICE_INLINE storage add(const storage &xs, const storage &ys) {
    storage rs = {};
    add_limbs<false>(xs, ys, rs);
    return reduce<REDUCTION_SIZE>(rs);
  }

  template <unsigned REDUCTION_SIZE = 1> static constexpr HOST_DEVICE_INLINE storage_wide add_wide(const storage_wide &xs, const storage_wide &ys) {
    storage_wide rs = {};
    add_limbs<false>(xs, ys, rs);
    return reduce_wide<REDUCTION_SIZE>(rs);
  }

  // return xs - ys with field operands
  template <unsigned REDUCTION_SIZE = 1> static HOST_DEVICE_INLINE storage sub(const storage &xs, const storage &ys) {
    storage rs = {};
    if (REDUCTION_SIZE == 0) {
      sub_limbs<false>(xs, ys, rs);
    } else {
      uint32_t carry = sub_limbs<true>(xs, ys, rs);
      if (carry == 0)
        return rs;
      const storage modulus = get_modulus<REDUCTION_SIZE>();
      add_limbs<false>(rs, modulus, rs);
    }
    return rs;
  }

  template <unsigned REDUCTION_SIZE = 1> static HOST_DEVICE_INLINE storage_wide sub_wide(const storage_wide &xs, const storage_wide &ys) {
    storage_wide rs = {};
    if (REDUCTION_SIZE == 0) {
      sub_limbs<false>(xs, ys, rs);
    } else {
      uint32_t carry = sub_limbs<true>(xs, ys, rs);
      if (carry == 0)
        return rs;
      const storage_wide modulus_squared = get_modulus_squared<REDUCTION_SIZE>();
      add_limbs<false>(rs, modulus_squared, rs);
    }
    return rs;
  }

#ifdef __CUDA_ARCH__

  // The following algorithms are adaptations of
  // http://www.acsel-lab.com/arithmetic/arith23/data/1616a047.pdf,
  // taken from https://github.com/z-prize/test-msm-gpu (under Apache 2.0 license)
  // and modified to use our datatypes.
  // We had our own implementation of http://www.acsel-lab.com/arithmetic/arith23/data/1616a047.pdf,
  // but the sppark versions achieved lower instruction count thanks to clever carry handling,
  // so we decided to just use theirs.

  static DEVICE_INLINE void mul_n(uint32_t *acc, const uint32_t *a, uint32_t bi, size_t n = TLC) {
#pragma unroll
    for (size_t i = 0; i < n; i += 2) {
      acc[i] = ptx::mul_lo(a[i], bi);
      acc[i + 1] = ptx::mul_hi(a[i], bi);
    }
  }

  static DEVICE_INLINE void cmad_n(uint32_t *acc, const uint32_t *a, uint32_t bi, size_t n = TLC) {
    acc[0] = ptx::mad_lo_cc(a[0], bi, acc[0]);
    acc[1] = ptx::madc_hi_cc(a[0], bi, acc[1]);
#pragma unroll
    for (size_t i = 2; i < n; i += 2) {
      acc[i] = ptx::madc_lo_cc(a[i], bi, acc[i]);
      acc[i + 1] = ptx::madc_hi_cc(a[i], bi, acc[i + 1]);
    }
    // return carry flag
  }

  static DEVICE_INLINE void madc_n_rshift(uint32_t *odd, const uint32_t *a, uint32_t bi) {
    constexpr uint32_t n = TLC;
#pragma unroll
    for (size_t i = 0; i < n - 2; i += 2) {
      odd[i] = ptx::madc_lo_cc(a[i], bi, odd[i + 2]);
      odd[i + 1] = ptx::madc_hi_cc(a[i], bi, odd[i + 3]);
    }
    odd[n - 2] = ptx::madc_lo_cc(a[n - 2], bi, 0);
    odd[n - 1] = ptx::madc_hi(a[n - 2], bi, 0);
  }

  static DEVICE_INLINE void mad_n_redc(uint32_t *even, uint32_t *odd, const uint32_t *a, uint32_t bi, bool first = false) {
    constexpr uint32_t n = TLC;
    constexpr auto modulus = CONFIG::modulus;
    const uint32_t *const MOD = modulus.limbs;
    if (first) {
      mul_n(odd, a + 1, bi);
      mul_n(even, a, bi);
    } else {
      even[0] = ptx::add_cc(even[0], odd[1]);
      madc_n_rshift(odd, a + 1, bi);
      cmad_n(even, a, bi);
      odd[n - 1] = ptx::addc(odd[n - 1], 0);
    }
    uint32_t mi = even[0] * INV;
    cmad_n(odd, MOD + 1, mi);
    cmad_n(even, MOD, mi);
    odd[n - 1] = ptx::addc(odd[n - 1], 0);
  }

  static DEVICE_INLINE void mad_row(uint32_t *odd, uint32_t *even, const uint32_t *a, uint32_t bi, size_t n = TLC) {
    cmad_n(odd, a + 1, bi, n - 2);
    odd[n - 2] = ptx::madc_lo_cc(a[n - 1], bi, 0);
    odd[n - 1] = ptx::madc_hi(a[n - 1], bi, 0);
    cmad_n(even, a, bi, n);
    odd[n - 1] = ptx::addc(odd[n - 1], 0);
  }

  static DEVICE_INLINE void qad_row(uint32_t *odd, uint32_t *even, const uint32_t *a, uint32_t bi, size_t n = TLC) {
    cmad_n(odd, a, bi, n - 2);
    odd[n - 2] = ptx::madc_lo_cc(a[n - 2], bi, 0);
    odd[n - 1] = ptx::madc_hi(a[n - 2], bi, 0);
    cmad_n(even, a + 1, bi, n - 2);
    odd[n - 1] = ptx::addc(odd[n - 1], 0);
  }

  static DEVICE_INLINE void multiply_raw(const storage &as, const storage &bs, storage_wide &rs) {
    const uint32_t *a = as.limbs;
    const uint32_t *b = bs.limbs;
    uint32_t *even = rs.limbs;
    __align__(8) uint32_t odd[2 * TLC - 2];
    mul_n(even, a, b[0]);
    mul_n(odd, a + 1, b[0]);
    mad_row(&even[2], &odd[0], a, b[1]);
    size_t i;
#pragma unroll
    for (i = 2; i < TLC - 1; i += 2) {
      mad_row(&odd[i], &even[i], a, b[i]);
      mad_row(&even[i + 2], &odd[i], a, b[i + 1]);
    }
    // merge |even| and |odd|
    even[1] = ptx::add_cc(even[1], odd[0]);
    for (i = 1; i < 2 * TLC - 2; i++)
      even[i + 1] = ptx::addc_cc(even[i + 1], odd[i]);
    even[i + 1] = ptx::addc(even[i + 1], 0);
  }

  static DEVICE_INLINE void sqr_raw(const storage &as, storage_wide &rs) {
    const uint32_t *a = as.limbs;
    uint32_t *even = rs.limbs;
    size_t i = 0, j;
    __align__(8) uint32_t odd[2 * TLC - 2];

    // perform |a[i]|*|a[j]| for all j>i
    mul_n(even + 2, a + 2, a[0], TLC - 2);
    mul_n(odd, a + 1, a[0], TLC);

#pragma unroll
    while (i < TLC - 4) {
      ++i;
      mad_row(&even[2 * i + 2], &odd[2 * i], &a[i + 1], a[i], TLC - i - 1);
      ++i;
      qad_row(&odd[2 * i], &even[2 * i + 2], &a[i + 1], a[i], TLC - i);
    }

    even[2 * TLC - 4] = ptx::mul_lo(a[TLC - 1], a[TLC - 3]);
    even[2 * TLC - 3] = ptx::mul_hi(a[TLC - 1], a[TLC - 3]);
    odd[2 * TLC - 6] = ptx::mad_lo_cc(a[TLC - 2], a[TLC - 3], odd[2 * TLC - 6]);
    odd[2 * TLC - 5] = ptx::madc_hi_cc(a[TLC - 2], a[TLC - 3], odd[2 * TLC - 5]);
    even[2 * TLC - 3] = ptx::addc(even[2 * TLC - 3], 0);

    odd[2 * TLC - 4] = ptx::mul_lo(a[TLC - 1], a[TLC - 2]);
    odd[2 * TLC - 3] = ptx::mul_hi(a[TLC - 1], a[TLC - 2]);

    // merge |even[2:]| and |odd[1:]|
    even[2] = ptx::add_cc(even[2], odd[1]);
    for (j = 2; j < 2 * TLC - 3; j++)
      even[j + 1] = ptx::addc_cc(even[j + 1], odd[j]);
    even[j + 1] = ptx::addc(odd[j], 0);

    // double |even|
    even[0] = 0;
    even[1] = ptx::add_cc(odd[0], odd[0]);
    for (j = 2; j < 2 * TLC - 1; j++)
      even[j] = ptx::addc_cc(even[j], even[j]);
    even[j] = ptx::addc(0, 0);

    // accumulate "diagonal" |a[i]|*|a[i]| product
    i = 0;
    even[2 * i] = ptx::mad_lo_cc(a[i], a[i], even[2 * i]);
    even[2 * i + 1] = ptx::madc_hi_cc(a[i], a[i], even[2 * i + 1]);
    for (++i; i < TLC; i++) {
      even[2 * i] = ptx::madc_lo_cc(a[i], a[i], even[2 * i]);
      even[2 * i + 1] = ptx::madc_hi_cc(a[i], a[i], even[2 * i + 1]);
    }
  }

  static DEVICE_INLINE void mul_by_1_row(uint32_t *even, uint32_t *odd, bool first = false) {
    uint32_t mi;
    constexpr auto modulus = CONFIG::modulus;
    const uint32_t *const MOD = modulus.limbs;
    if (first) {
      mi = even[0] * INV;
      mul_n(odd, MOD + 1, mi);
      cmad_n(even, MOD, mi);
      odd[TLC - 1] = ptx::addc(odd[TLC - 1], 0);
    } else {
      even[0] = ptx::add_cc(even[0], odd[1]);
      // we trust the compiler to *not* touch the carry flag here
      // this code sits in between two "asm volatile" instructions which should guarantee that nothing else interferes with the carry flag
      mi = even[0] * INV;
      madc_n_rshift(odd, MOD + 1, mi);
      cmad_n(even, MOD, mi);
      odd[TLC - 1] = ptx::addc(odd[TLC - 1], 0);
    }
  }

  // Performs Montgomery reduction on a storage_wide input. Input value must be in the range [0, mod*2^(32*TLC)).
  // Does not implement an in-place reduce<REDUCTION_SIZE> epilogue. If you want to further reduce the result,
  // call reduce<whatever>(xs.get_lo()) after the call to redc_wide_inplace.
  static DEVICE_INLINE void redc_wide_inplace(storage_wide &xs) {
    uint32_t *even = xs.limbs;
    // Yields montmul of lo TLC limbs * 1.
    // Since the hi TLC limbs don't participate in computing the "mi" factor at each mul-and-rightshift stage,
    // it's ok to ignore the hi TLC limbs during this process and just add them in afterward.
    uint32_t odd[TLC];
    size_t i;
#pragma unroll
    for (i = 0; i < TLC; i += 2) {
      mul_by_1_row(&even[0], &odd[0], i == 0);
      mul_by_1_row(&odd[0], &even[0]);
    }
    even[0] = ptx::add_cc(even[0], odd[1]);
#pragma unroll
    for (i = 1; i < TLC - 1; i++)
      even[i] = ptx::addc_cc(even[i], odd[i + 1]);
    even[i] = ptx::addc(even[i], 0);
    // Adds in (hi TLC limbs), implicitly right-shifting them by TLC limbs as if they had participated in the
    // add-and-rightshift stages above.
    xs.limbs[0] = ptx::add_cc(xs.limbs[0], xs.limbs[TLC]);
#pragma unroll
    for (i = 1; i < TLC - 1; i++)
      xs.limbs[i] = ptx::addc_cc(xs.limbs[i], xs.limbs[i + TLC]);
    xs.limbs[TLC - 1] = ptx::addc(xs.limbs[TLC - 1], xs.limbs[2 * TLC - 1]);
  }

  static DEVICE_INLINE void montmul_raw(const storage &a_in, const storage &b_in, storage &r_in) {
    constexpr uint32_t n = TLC;
    constexpr auto modulus = CONFIG::modulus;
    const uint32_t *const MOD = modulus.limbs;
    const uint32_t *a = a_in.limbs;
    const uint32_t *b = b_in.limbs;
    uint32_t *even = r_in.limbs;
    __align__(8) uint32_t odd[n + 1];
    size_t i;
#pragma unroll
    for (i = 0; i < n; i += 2) {
      mad_n_redc(&even[0], &odd[0], a, b[i], i == 0);
      mad_n_redc(&odd[0], &even[0], a, b[i + 1]);
    }
    // merge |even| and |odd|
    even[0] = ptx::add_cc(even[0], odd[1]);
#pragma unroll
    for (i = 1; i < n - 1; i++)
      even[i] = ptx::addc_cc(even[i], odd[i + 1]);
    even[i] = ptx::addc(even[i], 0);
    // final reduction from [0, 2*mod) to [0, mod) not done here, instead performed optionally in mul_device wrapper
  }

  // Returns xs * ys without Montgomery reduction.
  template <unsigned REDUCTION_SIZE = 1> static constexpr DEVICE_INLINE storage_wide mul_wide(const storage &xs, const storage &ys) {
    // Forces us to think more carefully about the last carry bit if we use a modulus with fewer than 2 leading zeroes of slack
    static_assert(!(CONFIG::modulus.limbs[TLC - 1] >> 30));
    storage_wide rs = {0};
    multiply_raw(xs, ys, rs);
    return reduce_wide<REDUCTION_SIZE>(rs);
  }

  // Performs Montgomery reduction on a storage_wide input. Input value must be in the range [0, mod*2^(32*TLC)).
  template <unsigned REDUCTION_SIZE = 1> static constexpr DEVICE_INLINE storage redc_wide(const storage_wide &xs) {
    storage_wide tmp{xs};
    redc_wide_inplace(tmp); // after reduce_twopass, tmp's low TLC limbs should represent a value in [0, 2*mod)
    return reduce<REDUCTION_SIZE>(tmp.get_lo());
  }

  template <unsigned REDUCTION_SIZE> static constexpr DEVICE_INLINE storage mul_device(const storage &xs, const storage &ys) {
    // Forces us to think more carefully about the last carry bit if we use a modulus with fewer than 2 leading zeroes of slack
    static_assert(!(CONFIG::modulus.limbs[TLC - 1] >> 30));
    storage rs = {0};
    montmul_raw(xs, ys, rs);
    return reduce<REDUCTION_SIZE>(rs);
  }

  template <unsigned REDUCTION_SIZE> static constexpr DEVICE_INLINE storage sqr_device(const storage &xs) {
    // Forces us to think more carefully about the last carry bit if we use a modulus with fewer than 2 leading zeroes of slack
    static_assert(!(CONFIG::modulus.limbs[TLC - 1] >> 30));
    storage_wide rs = {0};
    sqr_raw(xs, rs);
    redc_wide_inplace(rs); // after reduce_twopass, tmp's low TLC limbs should represent a value in [0, 2*mod)
    return reduce<REDUCTION_SIZE>(rs.get_lo());
  }

#endif // #ifdef __CUDA_ARCH__

  template <unsigned REDUCTION_SIZE> static constexpr HOST_INLINE storage mul_host(const storage &xs, const storage &ys) {
    const uint32_t *x = xs.limbs;
    const uint32_t *y = ys.limbs;
    constexpr storage ms = CONFIG::modulus;
    const uint32_t *const n = ms.limbs;
    constexpr uint32_t q = CONFIG::inv;
    uint32_t t[TLC + 2] = {};
    for (const uint32_t y_limb : ys.limbs) {
      uint32_t carry = 0;
      for (unsigned i = 0; i < TLC; i++)
        t[i] = host_math::madc_cc(x[i], y_limb, t[i], carry);
      t[TLC] = host_math::add_cc(t[TLC], carry, carry);
      t[TLC + 1] = carry;
      carry = 0;
      const uint32_t m = q * t[0];
      host_math::madc_cc(m, n[0], t[0], carry);
      for (unsigned i = 1; i < TLC; i++)
        t[i - 1] = host_math::madc_cc(m, n[i], t[i], carry);
      t[TLC - 1] = host_math::add_cc(t[TLC], carry, carry);
      t[TLC] = t[TLC + 1] + carry;
    }
    const storage rs = *reinterpret_cast<storage *>(t);
    return reduce<REDUCTION_SIZE>(rs);
  }

  // return xs * ys with field operands
  // Device path adapts http://www.acsel-lab.com/arithmetic/arith23/data/1616a047.pdf to use IMAD.WIDE.
  // Host path uses CIOS.
  template <unsigned REDUCTION_SIZE = 1> static constexpr HOST_DEVICE_INLINE storage mul(const storage &xs, const storage &ys) {
#ifdef __CUDA_ARCH__
    return mul_device<REDUCTION_SIZE>(xs, ys);
#else
    return mul_host<REDUCTION_SIZE>(xs, ys);
#endif
  }

  // convert field to montgomery form
  template <unsigned REDUCTION_SIZE = 1> static constexpr HOST_DEVICE_INLINE storage to_montgomery(const storage &xs) {
    constexpr storage r2 = CONFIG::r2;
    return mul<REDUCTION_SIZE>(xs, r2);
  }

  // convert field from montgomery form
  template <unsigned REDUCTION_SIZE = 1> static constexpr HOST_DEVICE_INLINE storage from_montgomery(const storage &xs) { return mul<REDUCTION_SIZE>(xs, {1}); }

  // return xs^2 with field operands
  template <unsigned REDUCTION_SIZE = 1> static constexpr HOST_DEVICE_INLINE storage sqr(const storage &xs) {
#ifdef __CUDA_ARCH__
    return sqr_device<REDUCTION_SIZE>(xs);
#else
    return mul_host<REDUCTION_SIZE>(xs, xs);
#endif
  }

// return 2*x with field operands
#ifdef __CUDA_ARCH__
  template <unsigned REDUCTION_SIZE> static constexpr DEVICE_INLINE storage dbl_device(const storage &xs) {
    const uint32_t *x = xs.limbs;
    storage rs = {};
    uint32_t *r = rs.limbs;
    r[0] = x[0] << 1;
#pragma unroll
    for (unsigned i = 1; i < TLC; i++)
      r[i] = __funnelshift_r(x[i - 1], x[i], 31);
    return reduce<REDUCTION_SIZE>(rs);
  }
#endif

  template <unsigned REDUCTION_SIZE> static constexpr HOST_INLINE storage dbl_host(const storage &xs) {
    const uint32_t *x = xs.limbs;
    storage rs = {};
    uint32_t *r = rs.limbs;
    r[0] = x[0] << 1;
    for (unsigned i = 1; i < TLC; i++)
      r[i] = (x[i] << 1) | (x[i - 1] >> 31);
    return reduce<REDUCTION_SIZE>(rs);
  }

  template <unsigned REDUCTION_SIZE = 1> static constexpr HOST_DEVICE_INLINE storage dbl(const storage &xs) {
#ifdef __CUDA_ARCH__
    return dbl_device<REDUCTION_SIZE>(xs);
#else
    return dbl_host<REDUCTION_SIZE>(xs);
#endif
  }

  // return x/2 with field operands
#ifdef __CUDA_ARCH__
  template <unsigned REDUCTION_SIZE> static constexpr DEVICE_INLINE storage div2_device(const storage &xs) {
    const uint32_t *x = xs.limbs;
    storage rs = {};
    uint32_t *r = rs.limbs;
#pragma unroll
    for (unsigned i = 0; i < TLC - 1; i++)
      r[i] = __funnelshift_rc(x[i], x[i + 1], 1);
    r[TLC - 1] = x[TLC - 1] >> 1;
    return reduce<REDUCTION_SIZE>(rs);
  }
#endif

  template <unsigned REDUCTION_SIZE> static constexpr HOST_INLINE storage div2_host(const storage &xs) {
    const uint32_t *x = xs.limbs;
    storage rs = {};
    uint32_t *r = rs.limbs;
    for (unsigned i = 0; i < TLC - 1; i++)
      r[i] = (x[i] >> 1) | (x[i + 1] << 31);
    r[TLC - 1] = x[TLC - 1] >> 1;
    return reduce<REDUCTION_SIZE>(rs);
  }

  template <unsigned REDUCTION_SIZE = 1> static constexpr HOST_DEVICE_INLINE storage div2(const storage &xs) {
#ifdef __CUDA_ARCH__
    return div2_device<REDUCTION_SIZE>(xs);
#else
    return div2_host<REDUCTION_SIZE>(xs);
#endif
  }

  // return -xs with field operand
  template <unsigned MODULUS_SIZE = 1> static constexpr HOST_DEVICE_INLINE storage neg(const storage &xs) {
    const storage modulus = get_modulus<MODULUS_SIZE>();
    storage rs = {};
    sub_limbs<false>(modulus, xs, rs);
    return rs;
  }

  // extract a given count of bits at a given offset from the field
  static constexpr DEVICE_INLINE uint32_t extract_bits(const storage &xs, const unsigned offset, const unsigned count) {
    const unsigned limb_index = offset / warpSize;
    const uint32_t *x = xs.limbs;
    const uint32_t low_limb = x[limb_index];
    const uint32_t high_limb = limb_index < (TLC - 1) ? x[limb_index + 1] : 0;
    uint32_t result = __funnelshift_r(low_limb, high_limb, offset);
    result &= (1 << count) - 1;
    return result;
  }

  template <unsigned REDUCTION_SIZE = 1, unsigned LAST_REDUCTION_SIZE = REDUCTION_SIZE>
  static constexpr HOST_DEVICE_INLINE storage mul(const unsigned scalar, const storage &xs) {
    storage rs = {};
    storage temp = xs;
    unsigned l = scalar;
    bool is_zero = true;
#ifdef __CUDA_ARCH__
#pragma unroll
#endif
    for (unsigned i = 0; i < 32; i++) {
      if (l & 1) {
        rs = is_zero ? temp : (l >> 1) ? add<REDUCTION_SIZE>(rs, temp) : add<LAST_REDUCTION_SIZE>(rs, temp);
        is_zero = false;
      }
      l >>= 1;
      if (l == 0)
        break;
      temp = dbl<REDUCTION_SIZE>(temp);
    }
    return rs;
  }

  static constexpr HOST_DEVICE_INLINE bool is_odd(const storage &xs) { return xs.limbs[0] & 1; }

  static constexpr HOST_DEVICE_INLINE bool is_even(const storage &xs) { return ~xs.limbs[0] & 1; }

  static constexpr HOST_DEVICE_INLINE bool lt(const storage &xs, const storage &ys) {
    storage dummy = {};
    uint32_t carry = sub_limbs<true>(xs, ys, dummy);
    return carry;
  }

  static constexpr HOST_DEVICE_INLINE storage inverse(const storage &xs) {
    if (is_zero(xs))
      return xs;
    constexpr storage one = {1};
    constexpr storage modulus = CONFIG::modulus;
    storage u = xs;
    storage v = modulus;
    storage b = CONFIG::r2;
    storage c = {};
    while (!eq(u, one) && !eq(v, one)) {
      while (is_even(u)) {
        u = div2(u);
        if (is_odd(b))
          add_limbs<false>(b, modulus, b);
        b = div2(b);
      }
      while (is_even(v)) {
        v = div2(v);
        if (is_odd(c))
          add_limbs<false>(c, modulus, c);
        c = div2(c);
      }
      if (lt(v, u)) {
        sub_limbs<false>(u, v, u);
        b = sub(b, c);
      } else {
        sub_limbs<false>(v, u, v);
        c = sub(c, b);
      }
    }
    return eq(u, one) ? b : c;
  }
};

typedef ff_dispatch_st<ff_config_p, inv_p> fd_p;
typedef ff_dispatch_st<ff_config_q, inv_q> fd_q;
